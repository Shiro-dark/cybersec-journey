## ğŸ§  Daily Log â€“ June 2, 2025

---

### ğŸ”¥ Focus

Crushing ML fundamentals using the Titanic dataset â€” diving into `pandas`, Jupyter, and your first classifier model.

---

### ğŸ› ï¸ What I Did

- **Explored Pandas for Data Wrangling:**
  - Loaded CSVs using `read_csv`
  - Used `.head()`, filtered, sorted, filled nulls
  - Mapped string values to integers (e.g. `Sex`, `Embarked`)

- **Analyzed Titanic Dataset:**
  - Identified survival patterns (rich girls survived, poor boys didnâ€™t)
  - Checked age groups (children had better survival odds)
  - Examined `SibSp`, `Parch`, `Pclass` for relational insights

- **Jupyter Notebook Work:**
  - Ran Python code cell-by-cell
  - Visualized data content clearly
  - Built clean and reproducible preprocessing steps

- **Built ML Model:**
  - Used `RandomForestClassifier` from `sklearn`
  - Preprocessed features (encoded + filled missing values)
  - Trained the model and made predictions
  - Submitted to Kaggle â†’ Scored **0.73684** on the first attempt

- **Kaggle Workflow:**
  - Downloaded challenge dataset
  - Trained, predicted, and submitted
  - Evaluated against public leaderboard

---

### ğŸ§  Lessons Learned

- Pandas = your SQL-meets-JS playground for data analysis
- Jupyter is perfect for building clean ML workflows step-by-step
- Feature preprocessing is **key** to solid model performance
- `RandomForestClassifier` is strong for starters
- First Kaggle score above 0.73 = youâ€™re not learning, youâ€™re executing

---

### âœ… Summary

- Hands-on pandas, sklearn, and data cleaning  
- First end-to-end ML pipeline built and tested  
- Cracked Titanic challenge with solid Kaggle score  
